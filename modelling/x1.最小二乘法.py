# 一、线性回归：用于预测一个连续地输出变量。
#
# 线性回归是一种基本的统计学方法，用于建立一个自变量（或多个自变量）和一个因变量之间的线性关系模型，以预测一个连续地输出变量。这个模型的形式可以表示为：
# y = β0 + β1x1 + β2x2 + ... + β_pxp + ε
# 其中，y 是因变量（也称为响应变量），x1， x2， ...， xp 是自变量（也称为特征变量），β0， β1， β2， ...， βp 是线性回归模型的系数，ε 是误差项
# 线性回归的目标是找到最优的系数 β0, β1, β2, ..., βp，使得模型预测的值与真实值之间的误差最小。这个误差通常用残差平方和来表示：
# RSS = Σ （yi - ŷi）^2
# 其中，yi 是真实的因变量值，ŷi 是通过线性回归模型预测的因变量值。线性回归模型的最小二乘估计法就是要找到一组系数，使得残差平方和最小。
# 线性回归可以通过多种方法来求解，其中最常用的方法是最小二乘法。最小二乘法就是要找到一组系数，使得残差平方和最小。最小二乘法可以通过矩阵运算来实现，具体地，系数的解可以表示为：
# β = （X'X）^（-1）X'y
# 其中，X 是自变量的矩阵，包括一个截距项和所有自变量的值，y 是因变量的向量。
# 线性回归在实际中的应用非常广泛，比如在金融、医学、工程、社会科学等领域中，都可以使用线性回归来预测和分析数据。

# 这个代码使用了 Numpy 库生成了一个包含 100 个样本的随机数据集，并使用 Scikit-learn 库的 LinearRegression 类创建了一个线性回归模型。
# 模型通过 fit（） 方法拟合数据，并通过 coef_ 和 intercept_ 属性访问模型的系数和截距项。
# 最后，代码使用 predict（） 方法预测了两个新数据点的结果，并打印出了预测结果。

import numpy as np
from sklearn.linear_model import LinearRegression

# 创建一个随机数据集
np.random.seed(0)
X = np.random.rand(100, 1)
y = 2 + 3 * X + np.random.rand(100, 1)

# 创建线性回归模型并拟合数据
model = LinearRegression()
model.fit(X, y)

# 打印模型的系数和截距项
print('Coefficients:', model.coef_)
print('Intercept:', model.intercept_)

# 预测新数据
X_new = np.array([[0.5], [1.0]])
y_new = model.predict(X_new)

# 打印预测结果
print('Predictions:', y_new)
